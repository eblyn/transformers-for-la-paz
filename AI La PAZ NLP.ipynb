{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AI La PAZ NLP.ipynb","provenance":[],"toc_visible":true,"mount_file_id":"1H1WHkUPXPwO3boMcAC6p4vkamR23wmAi","authorship_tag":"ABX9TyOm6enf4m0/HfADtcM+EBqg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"aJJely_LNJ3V"},"source":["# Ejercicios para la clase práctica y tarea:"]},{"cell_type":"markdown","metadata":{"id":"yG1AUMRwRVbC"},"source":["## Ejercicio 1: Traducción\n","Multilingual Machine Translation (MMT). En este ejercicio entrenaremos... un modelo de traducción del lenguaje natural, para los idiomas inglés y alemán.\n","\n","### Datos:\n","\n","https://arxiv.org/pdf/2010.11125.pdf\n","pagina 7 para coleccion de datasets de traduccion\n","\n","### Objetivo:\n","\n","que los estudiantes sean capaces de reconocer:\n","* la arquitectura básica de los transformer, encoder, decoder, positional ...\n","* cómo y para qué usar la máscara \n","* cómo entrenarlos y \n","* cómo utilizarlos\n","\n"]},{"cell_type":"markdown","metadata":{"id":"197IJB7ZVE2-"},"source":["### Data Processing\n","Para el primer ejercicio utilizaremos ***torchtext***, que es la librería de pytorch que ofrece utilidades y datos para trabajar con NLP. En particular utilizaremos para este ejemplo:\n","\n","*   get_tokenizer del torchtext.data.utils\n","*   Vocab del torchtext.vocab\n","*   download_from_url, extract_archive del torchtext.utils\n","\n","En este primer ejemplo la vamos a usar para el procesamiento de los datos en función del modelo de traducción del lenguaje natural. \n","\n","Primero vamos a tokenizar una oración de texto sin formato, construiremos el vocabulario y convertiremos los tokens en tensores numéricos\n"]},{"cell_type":"markdown","metadata":{"id":"E_vt_hFsVMjL"},"source":["Correr esta celda primero y luego reiniciar el entorno de ejecución. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ntv0_Sebmv2e","executionInfo":{"status":"ok","timestamp":1633618137280,"user_tz":240,"elapsed":12973,"user":{"displayName":"Evelin Fonseca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUG3DRgnMYlpEgofpT025kW1hujYvTQm45vWr7tz0=s64","userId":"13694612035696634831"}},"outputId":"b2dd0caf-dd05-4f1d-f819-4fef848bebea"},"source":["! python -m spacy download de_core_news_sm"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting de_core_news_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9 MB)\n","\u001b[K     |████████████████████████████████| 14.9 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.62.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.8.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2021.5.30)\n","Building wheels for collected packages: de-core-news-sm\n","  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-py3-none-any.whl size=14907055 sha256=72ace8ffd30580f17cfe3d018bc3035083d003b2e906c63dbfa95ff98de4a930\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-1j5z4j1r/wheels/00/66/69/cb6c921610087d2cab339062345098e30a5ceb665360e7b32a\n","Successfully built de-core-news-sm\n","Installing collected packages: de-core-news-sm\n","Successfully installed de-core-news-sm-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('de_core_news_sm')\n"]}]},{"cell_type":"code","metadata":{"id":"_fvYYalYRUwf","executionInfo":{"status":"ok","timestamp":1633618204632,"user_tz":240,"elapsed":4622,"user":{"displayName":"Evelin Fonseca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUG3DRgnMYlpEgofpT025kW1hujYvTQm45vWr7tz0=s64","userId":"13694612035696634831"}}},"source":["import math\n","import torchtext\n","import torch\n","import torch.nn as nn\n","from torchtext.data.utils import get_tokenizer\n","from collections import Counter\n","from torchtext.vocab import Vocab\n","from torchtext.utils import download_from_url, extract_archive\n","from torch import Tensor\n","import io\n","import time\n","\n","torch.manual_seed(0)\n","torch.use_deterministic_algorithms(True)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","BATCH_SIZE = 128"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"PHxKw25KjZBF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633618209230,"user_tz":240,"elapsed":2728,"user":{"displayName":"Evelin Fonseca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUG3DRgnMYlpEgofpT025kW1hujYvTQm45vWr7tz0=s64","userId":"13694612035696634831"}},"outputId":"e7772b03-9865-48fc-a9f1-232d8c176710"},"source":["url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n","\n","train_urls = ('train.de.gz', 'train.en.gz')\n","val_urls = ('val.de.gz', 'val.en.gz')\n","test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n","\n","train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n","val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n","test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["train.de.gz: 100%|██████████| 637k/637k [00:00<00:00, 15.2MB/s]\n","train.en.gz: 100%|██████████| 569k/569k [00:00<00:00, 14.2MB/s]\n","val.de.gz: 100%|██████████| 24.7k/24.7k [00:00<00:00, 7.86MB/s]\n","val.en.gz: 100%|██████████| 21.6k/21.6k [00:00<00:00, 3.01MB/s]\n","test_2016_flickr.de.gz: 100%|██████████| 22.9k/22.9k [00:00<00:00, 6.06MB/s]\n","test_2016_flickr.en.gz: 100%|██████████| 21.1k/21.1k [00:00<00:00, 3.48MB/s]\n"]}]},{"cell_type":"code","metadata":{"id":"vsLRKculjlab","executionInfo":{"status":"ok","timestamp":1633618213488,"user_tz":240,"elapsed":2696,"user":{"displayName":"Evelin Fonseca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUG3DRgnMYlpEgofpT025kW1hujYvTQm45vWr7tz0=s64","userId":"13694612035696634831"}}},"source":["from spacy.lang.en import English\n","\n","de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n","en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bk3gMm9tVe6r"},"source":["Esta próxima celda se demora 27 min con gpu, tenemos que buscar una mejor solución para la clase, parece que lo que más se demora es el counter.update(...).Para luego la solución es darle el vocabulario ya salvado: Leer https://github.com/bentrevett/pytorch-sentiment-analysis/issues/40"]},{"cell_type":"code","metadata":{"id":"nmIww6wrUOei","executionInfo":{"status":"ok","timestamp":1633620109886,"user_tz":240,"elapsed":4447,"user":{"displayName":"Evelin Fonseca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUG3DRgnMYlpEgofpT025kW1hujYvTQm45vWr7tz0=s64","userId":"13694612035696634831"}}},"source":["def build_vocab(filepath, tokenizer):\n","  counter = Counter()\n","  with io.open(filepath, encoding=\"utf8\") as f:\n","    for string_ in f:\n","      counter.update(tokenizer(string_))\n","  vocab = Vocab(counter)\n","  vocab.specials = ['<unk>', '<pad>', '<bos>', '<eos>']\n","  # vocab = Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n","  return vocab\n","\n","de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n","en_vocab = build_vocab(train_filepaths[1], en_tokenizer)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29hto1RHWTsl","executionInfo":{"status":"ok","timestamp":1633620287858,"user_tz":240,"elapsed":121,"user":{"displayName":"Evelin Fonseca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUG3DRgnMYlpEgofpT025kW1hujYvTQm45vWr7tz0=s64","userId":"13694612035696634831"}},"outputId":"a7ec4390-bb27-483e-e3ef-31d6a5ea5686"},"source":["de_vocab.type"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method Module.type of Vocab()>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"qwUvvCEUqPBY"},"source":["folder = '/content/drive/MyDrive/Colab Notebooks'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"riGJoyFspv3K"},"source":["torch.save(de_vocab, f'{folder}/de_vocab.pth')\n","torch.save(en_vocab, f'{folder}/en_vocab.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4pddDF0p78R"},"source":["de_vocab = torch.load(f'{folder}/de_vocab.pth')\n","en_vocab = torch.load(f'{folder}/en_vocab.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fL5QNHOYv_N2"},"source":["tiempo de ejecución: 28 min Esto tb se demora otro montón ... probablemente tenga más sentido salval estos datos que los vocabularios. Dejarle los códigos a los muchachos para que los corran por su cuenta y tengan referencia para la tarea peroen la clase solo darle el código de cargar los datos"]},{"cell_type":"code","metadata":{"id":"z3sZV1_IUX97"},"source":["def data_process(filepaths):\n","  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n","  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n","  data = []\n","  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n","    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de.rstrip(\"\\n\"))],\n","                            dtype=torch.long)\n","    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en.rstrip(\"\\n\"))],\n","                            dtype=torch.long)\n","    data.append((de_tensor_, en_tensor_))\n","  return data\n","\n","\n","train_data = data_process(train_filepaths)\n","val_data = data_process(val_filepaths)\n","test_data = data_process(test_filepaths)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sAN0qaipVpfR"},"source":["### DataLoader\n","The last torch specific feature we’ll use is the DataLoader, which is easy to use since it takes the data as its first argument. Specifically, as the docs say: DataLoader combines a dataset and a sampler, and provides an iterable over the given dataset. The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.\n","\n","Please pay attention to collate_fn (optional) that merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset."]},{"cell_type":"code","metadata":{"id":"YwNT603tUfpQ"},"source":["PAD_IDX = de_vocab['<pad>']\n","BOS_IDX = de_vocab['<bos>']\n","EOS_IDX = de_vocab['<eos>']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VpjQKrOEV2lQ"},"source":["from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","\n","def generate_batch(data_batch):\n","  de_batch, en_batch = [], []\n","  for (de_item, en_item) in data_batch:\n","    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n","    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n","  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n","  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n","  return de_batch, en_batch\n","\n","train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n","                        shuffle=True, collate_fn=generate_batch)\n","valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n","                        shuffle=True, collate_fn=generate_batch)\n","test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n","                       shuffle=True, collate_fn=generate_batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q-awIObc18dZ"},"source":["\n","\n","torch.save(train_iter, f'{folder}/train_iter')\n","torch.save(valid_iter, f'{folder}/valid_iter')\n","torch.save(test_iter, f'{folder}/test_iter')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ww-v2mSX2NPD"},"source":["train_iter = torch.load(f'{folder}/train_iter')\n","valid_iter = torch.load(f'{folder}/valid_iter')\n","test_iter = torch.load(f'{folder}/test_iter')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bWD_JhM-WcjR"},"source":["### Transformer!\n","Transformer is a Seq2Seq model introduced in “Attention is all you need” paper for solving machine translation task. Transformer model consists of an encoder and decoder block each containing fixed number of layers.\n","\n","Encoder processes the input sequence by propogating it, through a series of Multi-head Attention and Feed forward network layers. The output from the Encoder referred to as memory, is fed to the decoder along with target tensors. Encoder and decoder are trained in an end-to-end fashion using teacher forcing technique."]},{"cell_type":"markdown","metadata":{"id":"VqMgPd2wWwUk"},"source":["Text tokens are represented by using token embeddings. Positional encoding is added to the token embedding to introduce a notion of word order."]},{"cell_type":"code","metadata":{"id":"n350Sf2NW81O"},"source":["class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size: int, emb_size):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.emb_size = emb_size\n","    def forward(self, tokens: Tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VdYuK1oTWx1L"},"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding)\n","\n","    def forward(self, token_embedding: Tensor):\n","        return self.dropout(token_embedding +\n","                            self.pos_embedding[:token_embedding.size(0),:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGNkLeQ2XFv_"},"source":["def generate_square_subsequent_mask(sz):\n","    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","def create_mask(src, tgt):\n","  src_seq_len = src.shape[0]\n","  tgt_seq_len = tgt.shape[0]\n","\n","  tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n","  src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n","\n","  src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n","  tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n","  return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DtS8yiA0WiyM"},"source":["from torch.nn import (TransformerEncoder, TransformerDecoder,\n","                      TransformerEncoderLayer, TransformerDecoderLayer)\n","\n","\n","class Seq2SeqTransformer(nn.Module):\n","    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n","                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n","                 dim_feedforward:int = 512, dropout:float = 0.1):\n","        super(Seq2SeqTransformer, self).__init__()\n","        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n","                                                dim_feedforward=dim_feedforward)\n","        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n","        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n","                                                dim_feedforward=dim_feedforward)\n","        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n","\n","        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n","        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n","        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n","        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n","\n","    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n","                tgt_mask: Tensor, src_padding_mask: Tensor,\n","                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n","        src_emb = self.positional_encoding(self.src_tok_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n","        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n","        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n","                                        tgt_padding_mask, memory_key_padding_mask)\n","        return self.generator(outs)\n","\n","    def encode(self, src: Tensor, src_mask: Tensor):\n","        return self.transformer_encoder(self.positional_encoding(\n","                            self.src_tok_emb(src)), src_mask)\n","\n","    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n","        return self.transformer_decoder(self.positional_encoding(\n","                          self.tgt_tok_emb(tgt)), memory,\n","                          tgt_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uRLNxf8GXE7o"},"source":["We create a subsequent word mask to stop a target word from attending to its subsequent words. We also create masks, for masking source and target padding tokens"]},{"cell_type":"markdown","metadata":{"id":"uxYy0GfLXOS3"},"source":["Define model parameters and instantiate model"]},{"cell_type":"code","metadata":{"id":"WaKqeXv8XSta"},"source":["SRC_VOCAB_SIZE = len(de_vocab)\n","TGT_VOCAB_SIZE = len(en_vocab)\n","EMB_SIZE = 512\n","NHEAD = 8\n","FFN_HID_DIM = 512\n","BATCH_SIZE = 128\n","NUM_ENCODER_LAYERS = 3\n","NUM_DECODER_LAYERS = 3\n","NUM_EPOCHS = 16\n","\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n","                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n","                                 FFN_HID_DIM)\n","\n","for p in transformer.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","transformer = transformer.to(device)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","optimizer = torch.optim.Adam(\n","    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYe8lAGcXW5g"},"source":["def train_epoch(model, train_iter, optimizer):\n","  model.train()\n","  losses = 0\n","  for idx, (src, tgt) in enumerate(train_iter):\n","      src = src.to(device)\n","      tgt = tgt.to(device)\n","\n","      tgt_input = tgt[:-1, :]\n","\n","      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","      logits = model(src, tgt_input, src_mask, tgt_mask,\n","                                src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","      optimizer.zero_grad()\n","\n","      tgt_out = tgt[1:,:]\n","      loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","      loss.backward()\n","\n","      optimizer.step()\n","      losses += loss.item()\n","  return losses / len(train_iter)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4HDvnz9fXafr"},"source":["def evaluate(model, val_iter):\n","  model.eval()\n","  losses = 0\n","  for idx, (src, tgt) in (enumerate(valid_iter)):\n","    src = src.to(device)\n","    tgt = tgt.to(device)\n","\n","    tgt_input = tgt[:-1, :]\n","\n","    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","    logits = model(src, tgt_input, src_mask, tgt_mask,\n","                              src_padding_mask, tgt_padding_mask, src_padding_mask)\n","    tgt_out = tgt[1:,:]\n","    loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","    losses += loss.item()\n","  return losses / len(val_iter)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VyGZZLmMXi0Y"},"source":["Entrenar el modelo"]},{"cell_type":"markdown","metadata":{"id":"nqFJUaZ2LJcQ"},"source":["de 19 a 10 min"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MwlnNCxIXfzM","executionInfo":{"status":"ok","timestamp":1622829928621,"user_tz":240,"elapsed":611991,"user":{"displayName":"Evelin Fonseca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUG3DRgnMYlpEgofpT025kW1hujYvTQm45vWr7tz0=s64","userId":"13694612035696634831"}},"outputId":"b66283f0-b21e-404d-de30-e06acf3e10b3"},"source":["for epoch in range(1, NUM_EPOCHS+1):\n","  start_time = time.time()\n","  train_loss = train_epoch(transformer, train_iter, optimizer)\n","  end_time = time.time()\n","  val_loss = evaluate(transformer, valid_iter)\n","  print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n","          f\"Epoch time = {(end_time - start_time):.3f}s\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 1, Train loss: 5.316, Val loss: 4.065, Epoch time = 35.168s\n","Epoch: 2, Train loss: 3.727, Val loss: 3.285, Epoch time = 36.240s\n","Epoch: 3, Train loss: 3.131, Val loss: 2.881, Epoch time = 38.013s\n","Epoch: 4, Train loss: 2.741, Val loss: 2.625, Epoch time = 38.112s\n","Epoch: 5, Train loss: 2.454, Val loss: 2.428, Epoch time = 37.905s\n","Epoch: 6, Train loss: 2.223, Val loss: 2.291, Epoch time = 38.161s\n","Epoch: 7, Train loss: 2.030, Val loss: 2.191, Epoch time = 38.123s\n","Epoch: 8, Train loss: 1.866, Val loss: 2.104, Epoch time = 38.119s\n","Epoch: 9, Train loss: 1.724, Val loss: 2.044, Epoch time = 38.123s\n","Epoch: 10, Train loss: 1.600, Val loss: 1.994, Epoch time = 38.075s\n","Epoch: 11, Train loss: 1.488, Val loss: 1.969, Epoch time = 38.171s\n","Epoch: 12, Train loss: 1.390, Val loss: 1.929, Epoch time = 37.945s\n","Epoch: 13, Train loss: 1.299, Val loss: 1.898, Epoch time = 38.172s\n","Epoch: 14, Train loss: 1.219, Val loss: 1.885, Epoch time = 38.110s\n","Epoch: 15, Train loss: 1.141, Val loss: 1.890, Epoch time = 37.984s\n","Epoch: 16, Train loss: 1.070, Val loss: 1.873, Epoch time = 38.075s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JliGAB7wXpu0"},"source":["The models trained using transformer architecture — train faster and converge to a lower validation loss compared to RNN models."]},{"cell_type":"code","metadata":{"id":"DvIkBzXwXs4N"},"source":["def greedy_decode(model, src, src_mask, max_len, start_symbol):\n","    src = src.to(device)\n","    src_mask = src_mask.to(device)\n","\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n","    for i in range(max_len-1):\n","        memory = memory.to(device)\n","        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                                    .type(torch.bool)).to(device)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim = 1)\n","        next_word = next_word.item()\n","\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == EOS_IDX:\n","          break\n","    return ys"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lGP8p-aqXx4G"},"source":["def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n","  model.eval()\n","  tokens = [BOS_IDX] + [src_vocab.stoi[tok] for tok in src_tokenizer(src)]+ [EOS_IDX]\n","  num_tokens = len(tokens)\n","  src = (torch.LongTensor(tokens).reshape(num_tokens, 1) )\n","  src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","  tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n","  return \" \".join([tgt_vocab.itos[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"KF_-TkHHX0ud","executionInfo":{"status":"ok","timestamp":1622829928830,"user_tz":240,"elapsed":214,"user":{"displayName":"Evelin Fonseca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUG3DRgnMYlpEgofpT025kW1hujYvTQm45vWr7tz0=s64","userId":"13694612035696634831"}},"outputId":"3f321a7a-858e-46a1-a21d-d395078428f8"},"source":["translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", de_vocab, en_vocab, de_tokenizer)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' A group of people stand in front of an igloo . '"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"YlxzTCW6X7Lk"},"source":["Output: A group of people stand in front of an igloo ."]},{"cell_type":"markdown","metadata":{"id":"no2m5fj2RWi9"},"source":["## Ejercicio 2: Vision transformer\n","\n","import timm.models.vision_transformer para vision transformer\n","\n","### Clip\n","\n","paper: https://arxiv.org/pdf/2103.00020.pdf\n","\n","code: https://github.com/openai/CLIP\n","\n","blog: https://openai.com/blog/clip/\n","\n","bueno eso sería para tener un modelo pretrained y hacer testing solo\n","\n","esto sería para image to seq (descriptions)\n","\n","### Deit:\n","https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification\n","\n","https://pytorch.org/tutorials/beginner/vt_tutorial.html\n","\n","\n","### Datos:\n","\n","MNIST\n","\n","Por el momento está fácil hacer un poco de entretenimiento con clip, y luego separarle un ejercicio para que completen con el modelo sin entrenar. Usando las funciones de la implementación de clip para transformer, cómo codificar la imagen, cómo codificar el texto, cómo hacer la clasificación.\n","\n","La idea del ejercicio puede ser que ellos jueguen con un ejemplo interactivo, preentrenado como clip. Luego el foco del ejercicio estaría en que encuentren cuál sería la diferencia entre:\n","\n","* la representación de imágenes como secuencias, para ser usadas en transformers\n","* cómo se haría la clasificación usando transformers\n","* cuál es la diferencia con el ejercicio anterior respecto a los objetivos: encoding, masks ...\n","\n","con cuál de los dos modelos anteriores es más fácil hacer este ejercicio para nosotros"]},{"cell_type":"code","metadata":{"id":"ayJsn4ExNGAj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sTBY6Ec5RhG2"},"source":["## Ejercicio 3: Sentiment Analysis\n","\n","### Datos:\n","\n","http://sabcorpus.linkeddata.es/ A corpus of tweets in Spanish annotated with the sentiment analysis towards brands.\n","\n","http://tass.sepln.org/tass_data/download.php A corpus of texts in Spanish tagged for sentiment analysis related tasks. It is divided into several subsets created for the various tasks proposed in the different editions through the years.\n","\n","Cómo sería el sistema de evaluación, hay que hacer el código completo y solo dejarle lo que tienen que completar o es dejarle el ejercicio para que ellos lo resuelvan, desde cero?\n","\n","### Objetivos, repasar los objetivos de la clase práctica en un ejercicio:\n","\n","* cómo se haría la clasificación usando transformers\n","* la arquitectura básica de los transformer, encoder, decoder, positional ...\n","* cómo y para qué usar la máscara \n","* cómo entrenarlos y \n","* cómo utilizarlos"]}]}